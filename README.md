# ‚ö° Forecast de Consumo de Energia - Copenhague, Dinamarca

![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=for-the-badge&logo=python)
![MLflow](https://img.shields.io/badge/MLflow-Tracking-blue?style=for-the-badge&logo=mlflow)
![XGBoost](https://img.shields.io/badge/XGBoost-Regressor-green?style=for-the-badge&logo=xgboost)
![Statsmodels](https://img.shields.io/badge/Statsmodels-ARIMA/SARIMA-red?style=for-the-badge)

## üìå Overview
O consumo de energia √© um dos mais fortes indicadores do desenvolvimento e potencial econ√¥mico de um pa√≠s. A previs√£o da demanda energ√©tica permite que ind√∫strias, governos e distribuidoras ajam de forma proativa para alocar recursos, evitar apag√µes e baratear os custos de opera√ß√£o.

Neste projeto de portf√≥lio, aplicamos metodologias de **Minera√ß√£o de Dados** e **Modelagem de S√©ries Temporais** para prever o consumo di√°rio de energia el√©trica do munic√≠pio de Copenhague (K√∏benhavn), capital da Dinamarca.

---

## üéØ Objetivos do Projeto
Desenvolver um modelo de alta precis√£o para prever os dados de consumo de energia num horizonte futuro de 60 dias, utilizando t√©cnicas que v√£o desde modelos cl√°ssicos de estat√≠stica (Box-Jenkins) at√© algoritmos robustos de *Machine Learning*.

**Principais t√≥picos abordados:**
- **Engenharia de Dados:** Coleta de dados via API e tratamento de _timestamps_.
- **An√°lise Explorat√≥ria (EDA):** Diferentes agrega√ß√µes, janelas deslizantes e an√°lise de sazonalidade.
- **S√©ries Temporais:** Avalia√ß√£o de estacionariedade (Teste ADF), diferencia√ß√£o e decomposi√ß√£o.
- **Modelagem Cl√°ssica:** Correlogramas (ACF e PACF), modelos ARIMA e SARIMA.
- **Machine Learning:** 
  - Engenharia de features temporais (*day, week, day of year*) e *lags* temporais.
  - Modelos Ridge, Random Forest e XGBoost Regressor.
  - Tuner de hiperpar√¢metros e *Cross Validation* para s√©ries temporais (`TimeSeriesSplit`).
- **Tracking de Experimentos:** Gerenciamento dos modelos e registro das m√©tricas (MAE e MAPE) utilizando **MLflow**.

---

## üìä Fonte de Dados e EDA
Os dados foram coletados publicamente atrav√©s da [API da Energinet (Dinamarca)](https://en.energinet.dk/energy-data/data-catalog/). A base possui alta granularidade (horas) e segrega o consumo em tr√™s grandes √°reas: **Ind√∫stria (Erhverv)**, **P√∫blico (Offentligt)** e **Privado (Privat)**.

### Descobertas da Avalia√ß√£o Gr√°fica e Sazonalidade:
1. **Perfis de Consumo Distintos:** O setor **Industrial** consumia volumes massivamente superiores e de forma muito mais constante que os demais. O setor p√∫blico era focado em volumes marginais e o privado apresentava forte assimetria durante o dia.
2. **Sazionalidade Di√°ria e Semanal:** Agregando para _dias_, notamos fortes picos de consumo provindos do setor _Privado_ aos finais de semana e redu√ß√µes expressivas na Ind√∫stria.
3. **Tend√™ncia Anual e Janelas Deslizantes:** Usando m√©dias m√≥veis (7 e 90 dias), identificou-se uma queda expressiva e repetida do consumo nos meses de veraneio europeu (como **Julho** e **Agosto**), al√©m de n√£o observar grandes tend√™ncias lineares de crescimento nos √∫ltimos anos.

<div align="center">
  <img src="images/output_65_12.png" width="80%" alt="M√©dias M√≥veis de Consumo">
</div>

---

## ü§ñ Modelagem e Resultados

Para determinar que o modelo escolhido √© de fato √∫til, estabelecemos um modelo de base (Baseline): **A m√©dia m√≥vel simples de 7 dias**, cuja simula√ß√£o resultou em um **Erro Percentual Absoluto M√©dio (MAPE) de ~6.02%**.

### 1. Modelos Cl√°ssicos (ARIMA e SARIMA)
A s√©rie temporal original n√£o era estacion√°ria ($p$-value do teste ADF > 0.05). Ap√≥s aplicar a primeira diferencia√ß√£o e usar os gr√°ficos de ACF e PACF, criamos arquiteturas iterativas. O melhor modelo cl√°ssico testado, que superou a baseline e previu corretamente a sazionalidade semanal, foi um modelo modular **SARIMA (1,1,1)(0,1,2)[7]**.

### 2. Modelos de Machine Learning (O Foco!)
Devido ao vasto volume de dados hist√≠ricos, os modelos de √°rvore provaram ser incrivelmente perform√°ticos. Dividimos a abordagem de features de duas formas:
- **Abordagem A:** Features extra√≠das por Data/Calend√°rio (*Dia da semana, Dia do ano, etc.*).
- **Abordagem B:** Features extra√≠das por Lags (janelas defasadas).

Testamos **Ridge Regression, Random Forest e XGBoost**. Os algoritmos n√£o lineares se sa√≠ram excepcionalmente bem. A *Abordagem A* (Calend√°rio) acompanhada do modelo XGBoost foi a preferida para simular a prova de hiperpar√¢metros, pois era mais adapt√°vel √†s din√¢micas futuras que a simples repeti√ß√£o do passado.

<div align="center">
  <img src="images/output_206_38.png" width="60%" alt="Feature Importance">
</div>
*Gr√°fico de Import√¢ncia de Features (XGBoost) revelando a enorme depend√™ncia temporal baseada no "Dia do Ano" (DayofYear).*

### ÔøΩ Tabela de Performance

| Modelo Aplicado | MAE | MAPE (%) |
| :--- | :---: | :---: |
| Baseline (M√©dia 7 dias) | 241,342.49 | 6.02 |
| ARIMA (Auto) | 236,505.08 | 5.93 |
| Regress√£o Ridge (Lags) | 103,340.00 | 2.53 |
| Random Forest (Lags) | 103,340.00 | 2.53 |
| XGBoost (Features Data) | 118,992.46 | 2.84 |

Ap√≥s a etapa de busca de hiperpar√¢metros (Hyperparameter Tuning com Cross Validation TimeSeriesSplit), o modelo final **XGBoost Tuned** foi testado resultando numa estabiliza√ß√£o de **MAPE em torno de 3.3%**.

> üí° Isso representa uma **redu√ß√£o de erro de quase 50%** comparado a m√©trica de previs√£o do Baseline de neg√≥cio! Em um cen√°rio de gest√£o energ√©tica, essa precis√£o reflete na enorme economia de recursos p√∫blicos.

---

## üîÆ Conclus√µes e Previs√£o Futura
Aplicando o modelo treinado a um cen√°rio de dados n√£o conhecidos, projetamos com sucesso o consumo dos 60 dias subsequentes. 

<div align="center">
  <img src="images/output_221_39.png" width="80%" alt="Previs√£o para 60 Dias">
</div>

Ao longo deste reposit√≥rio, ficou provado o impacto gigantesco da explora√ß√£o atenciosa dos dados para identifica√ß√£o de sazionalidades, bem como do poder da Modelagem de Dados Moderna sobre a previs√£o est√°tica do passado.

### Pr√≥ximos Passos (Extras)
- Realizar deploy das predi√ß√µes com integra√ß√£o a uma interface Flask / FastAPI.
- Rodar experimenta√ß√µes adicionais utilizando **Prophet (Meta)** ou Algoritmos de Deep Learning.
- Expandir a previs√£o para m√∫ltiplas regi√µes ou cruzamento de todos os modais de forma integrada.
